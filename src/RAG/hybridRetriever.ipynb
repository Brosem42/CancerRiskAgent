{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b00de5a0",
   "metadata": {},
   "source": [
    "# Safely Store ENV variables for Gemini, Chroma DB connection, and vector embeddings\n",
    "### Additionl techniques: cleared memory of any existing variables to avoid accidental exposure. Performed due to significant security risk of exposing PII and PHI for healthcare data.\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3487021f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install -U langchain-google-vertexai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f82a3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install -U llama_index chromadb google-generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be341c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install llama-index llama-index-vector-stores-chroma llama-index-embeddings-huggingface llama-index-retrievers-bm25\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222d0294",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install rank-bm25\n",
    "#! pip install langchain-classic rank_bm25 langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58778aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports \n",
    "# # load dotenv + imports for retriever tool\n",
    "from getpass import getpass\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_chroma import Chroma \n",
    "from langchain.tools import tool\n",
    "import langchainhub as hub\n",
    "import chromadb\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "import vertexai\n",
    "from vertexai.language_models import TextEmbeddingModel\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "from langchain_core.runnables import RunnableLambda, RunnableSequence, RunnableMap, RunnablePassthrough, RunnableParallel\n",
    "from langchain_core.runnables import Runnable\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.core import StorageContext, VectorStoreIndex, Document\n",
    "from llama_index.retrievers.bm25 import BM25Retriever\n",
    "from llama_index.core.retrievers import QueryFusionRetriever\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b43dd47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "load_dotenv()\n",
    "\n",
    "api_key = os.getenv(\"CHROMA_API_KEY\") or getpass(\"Paste CHROMA_API_KEY: \")\n",
    "tenant = os.getenv(\"CHROMA_TENANT\") or input(\"Enter CHROMA_TENANT: \")\n",
    "database = os.getenv(\"CHROMA_DATABASE\") or input(\"Enter CHROMA_DATABASE: \")\n",
    "gemini_key = os.getenv(\"GEMINI_API_KEY\") or getpass(\"Enter GEMINI_API_KEY: \")\n",
    "vertex_ai_key = os.getenv(\"GOOGLE_APPLICATION_CREDENTIALS\") or getpass(\"Enter GOOGLE_APPLICATION_CREDENTIALS: \")\n",
    "\n",
    "os.environ[\"GOOGLE_API_KEY\"] = gemini_key\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = vertex_ai_key\n",
    "\n",
    "# Load environment variables from chromaDB --CloudClient\n",
    "client = chromadb.CloudClient(\n",
    "    api_key=api_key, \n",
    "    tenant=tenant, \n",
    "    database=database)\n",
    "\n",
    "#importing Gemini for embedding, checking dimensions for retriever\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"gemini-embedding-001\")\n",
    "embeddings.embed_query(\"NG12 Cancer Risk Assessor guidelines\")\n",
    "\n",
    "#vertexai init\n",
    "embedding = GoogleGenerativeAIEmbeddings(\n",
    "    model=\"gemini-embedding-001\",\n",
    "    project=\"CancerRiskAgent\",\n",
    "    vertexai=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb68de93",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get collection name from chromaDB\n",
    "COLLECTION_NAME = \"ng12\"\n",
    "chroma_collection = client.get_collection(name=COLLECTION_NAME)\n",
    "\n",
    "#vectore store\n",
    "vector_store = ChromaVectorStore(\n",
    "    chroma_collection=chroma_collection\n",
    ")\n",
    "\n",
    "# storage context \n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "storage_context.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c37376",
   "metadata": {},
   "source": [
    "# Enhancing Retrieval in Responses by Improving Contextual Understanding and Relevance with Llama_index\n",
    "---\n",
    "#### **Tradeoffs:**\n",
    "- Originally, my plan was to use the same framework for both retrievers, then fetch the data from my vector store in chroma. However, I found that my BM25 retriever simply would not return any data for either framework--langchain or llama_index. Later on, I found out that due to how the BM25-Retriever architecture is built, the keyword search library needs access to pull data from **raw text nodes**. This does not align with my current Chroma vector store database architecture, which is built **with stored embeddings--not original text**. Hence, why orginally no data was being returned in my prior iteration. \n",
    "#### **How I solved for this:**\n",
    "To mitigate for this, I decided to change my framework and use llama_index for BM25/lexical retriever; it provides a much more seamless integration with the Chroma vector store architecture. Then, I used llama_index to pass my list of nodes directly to the BM25 retriever, which solved the problem.\n",
    "#### **Improvement techniques with Llama_index:**\n",
    "- Use raw text nodes for BM25 retrieval to ensure compatibility with the keyword search library.\n",
    "- Implement response enhancement by providing additional context or *page_content* for my document queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "29ba2418",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding response enhancement for generated outputs\n",
    "documents = [\n",
    "    Document(text=\"Shortness of breath with cough or fatigue or chest pain or weight loss or appetite loss (unexplained), 40 and over: possible cancer Lung or mesothelioma\", metadata={\"referral\": \"Urgent\", \"source\": \"Suspected cancer: recognition and referral (NG12) 2026\", \"page\": \"52\"}),\n",
    "    Document(text=\"Bleeding, bruising or petechiae, unexplained: possible cancer Leukaemia\", metadata={\"referral\": \"Very urgent\", \"source\": \"Suspected cancer: recognition and referral (NG12) 2026\", \"page\": \"43\"}),\n",
    "    Document(text=\"Fracture unexplained, 60 and over: possible cancer Myeloma\", metadata={\"referral\": \"Unexplained\", \"source\": \"Suspected cancer: recognition and referral (NG12) 2026\", \"page\": \"55\"}),\n",
    "    Document(text=\"Refer people using a suspected cancer pathway referral for oesophageal cancer if they: have dysphagia or, are aged 55 and over, with weight loss, and they have any of the following: upper abdominal pain, reflux, dyspepsia. [2015, amended 2025]\", metadata={\"referral\": \"Suspected cancer pathway referral\", \"source\": \"Suspected cancer: recognition and referral (NG12) 2026\", \"page\": \"11\"}),\n",
    "    Document(text=\"Skin lesion that raises the suspicion of a basal cell carcinoma: possible cancer Basal cell carcinoma  \", metadata={\"referral\": \"Raises the suspicion of\", \"source\": \"Suspected cancer: recognition and referral (NG12) 2026\", \"page\": \"58\"}),\n",
    "    Document(text=\"Urinary urgency or frequency, increased and persistent or frequent, particularly more than 12 times per month in women, especially if 50 and over: possible cancer Ovarian\", metadata={\"referral\": \"Persistent\", \"source\": \"Suspected cancer: recognition and referral (NG12) 2026\", \"page\": \"60\"}),\n",
    "    Document(text=\"Upper abdominal pain with low haemoglobin levels or raised platelet count or nausea or vomiting, 55 and over: possible cancer Oesophageal or stomach \", metadata={\"referral\": \"Non-urgent\", \"source\": \"Suspected cancer: recognition and referral (NG12) 2026\", \"page\": \"40\"}),\n",
    "    Document(text=\"Petechiae unexplained in children and young people: possible cancer Leukaemia\", metadata={\"referral\": \"Immediate\", \"source\": \"Suspected cancer: recognition and referral (NG12) 2026\", \"page\": \"74\"})\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4d6777",
   "metadata": {},
   "source": [
    "# Stopping here for now--will continue to add more techniques "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415c9e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "VectorStoreIndex.from_documents(documents, storage_context=storage_context, embeddings=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695a7d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = VectorStoreIndex.from_vector_store(vector_store, \n",
    "                                           storage_context=storage_context,\n",
    "                                           embeddings=embeddings)\n",
    "\n",
    "vector_retriever = index.as_retriever(similarity_top_k=5)\n",
    "bm25_retriever = BM25Retriever.from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d6fb01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# semantic retriever\n",
    "vector_retriever = vector_store.as_retriever(search_kwargs={\"k\": 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8254fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pull data from vector to create documents for BM25\n",
    "doc_data = vector_store.get()\n",
    "documents = [Document(page_content=d, metadata=m) for d, m in zip(doc_data['documents'], doc_data['metadatas'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ab4b16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9640ff6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lexical retriever\n",
    "bm25_retriever = BM25Retriever.from_documents(documents)\n",
    "bm25_retriever.k = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f2932e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#semantic retriever/ vector retriever\n",
    "vector_retriever = new_vs.as_retriever(search_kwargs={\"k\": 5})\n",
    "\n",
    "# keyword with BM25 retriever\n",
    "new_data = new_vs.get(include=[\"documents\", \"metadatas\"])\n",
    "documents_from_chroma = [\n",
    "    Document(page_content=text, metadata=(meta or {}))\n",
    "    for text, meta in zip(new_data[\"documents\"], new_data[\"metadatas\"])\n",
    "]\n",
    "\n",
    "bm25_retriever = BM25Retriever.from_documents(documents_from_chroma)\n",
    "bm25_retriever.k = 20\n",
    "\n",
    "\n",
    "#hybrid retriever \n",
    "hybrid_retriever = EnsembleRetriever(\n",
    "    retrievers=[vector_retriever, bm25_retriever],\n",
    "    weights=[0.5, 0.5]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3fbc2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper for context engineering\n",
    "def get_cancer_context(docs):\n",
    "    context = []\n",
    "    for i, doc in enumerate(docs):\n",
    "        page = doc.metatdata.get(\"page\", \"?\")\n",
    "        block = f\"[Source: Page {page}] {doc.page_content}\"\n",
    "        context.append(block)\n",
    "    return \"\\n---\\n\".join(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0797fb8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install -qU langchain langchain-core langchain-community\n",
    "#! pip install -U langchain langchain-community langchain-chroma rank_bm25 flashrank"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871b5c7a",
   "metadata": {},
   "source": [
    "# Hybrid Retrieval Search with BM25 semantic + keyword search\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "091aba63",
   "metadata": {},
   "source": [
    "### Semantic Retriever\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ffebad",
   "metadata": {},
   "source": [
    "###  Lexical Retriever with BM25\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbad987c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "collection_data = vectorstore.get()\n",
    "\n",
    "documents_from_chroma = [\n",
    "    Document(page_content=doc, metadata=meta)\n",
    "    for doc, meta in zip(\n",
    "        collection_data[\"documents\"],\n",
    "        collection_data[\"metadatas\"]\n",
    "    )\n",
    "]\n",
    "\n",
    "bm25_retriever = BM25Retriever.from_documents(documents_from_chroma)\n",
    "bm25_retriever.k = 20\n",
    "\n",
    "hybrid_retriever = EnsembleRetriever(\n",
    "    retrievers=[vector_retriever, bm25_retriever],\n",
    "    weights=[0.5, 0.5]\n",
    ")\n",
    "\n",
    "results = hybrid_retriever.invoke(\n",
    "    \"Urgent referral criteria for lung cancer\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89405f2",
   "metadata": {},
   "source": [
    "### **Combine retrievers with EnsembleRetriever + BM25 for hybrid retrieval**\n",
    "---\n",
    "### **Tradeoffs**: \n",
    "\n",
    "#### **Balance vs. Imbalanced Weights for retrievers:**\n",
    "---\n",
    "\n",
    "**Why did I use 0.5 semantic and 0.5 lexical for the weights in my hybrid retriever?**\n",
    "\n",
    "**Reasoning:** \n",
    "I wanted to balance the weight + importance of both the semantic and lexical retrieval meaning. The way the query's are formatted is unknown to me at this stage, so the best bet was to start with an equal balance and then adjust as needed.\n",
    "\n",
    "Also, to compensate for the equal balance, I'm adding a lightweight **re-ranker LLM FlashRank**, it uses a pointwise reranking system. I chose this because it's fast, efficient, and runs FREE locally on my machine. At the end of the day, I would say that the semantic retriever will likely be more important for the types of queries that will be used in my clinical application, but I wanted to give the lexical retriever a fair chance to contribute as well.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454818bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# langchain depreacted the get relevant docs method for Ensemble Retrievers, updated method uses runnable utilities\n",
    "from langchain_core.runnables import RunnableLambda, RunnableSequence, RunnableMap, RunnablePassthrough, RunnableParallel\n",
    "from langchain_core.runnables import Runnable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc721263",
   "metadata": {},
   "source": [
    "# Perform Context Engineering to prepare for Agentic Reasoning\n",
    "---\n",
    "### Tradeoffs:\n",
    "\n",
    "**Why did I choose this approach for context engineering?**\n",
    "\n",
    "**Reasoning:** \n",
    "At first, my goal was to return a single string of relevant documents. However, I realized that my agent would have trouble parsing my actual data in production. Based on the way the data is formatted, I had to make some adjustments, and go a bit deeper with formatting. Also, one of the key constraints in building this agent included a task to cite the specific sources within the relevant data retrieved from the NG12 documents. I could have kept my single string, however, there would have been significant fine-tuning and overhead later. Performing context engineering was the only plausible to way achieve this goal. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc77bd73",
   "metadata": {},
   "source": [
    "#### Adding Clinical Context Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5cf3f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clinical context tooling for response -pg143\n",
    "from langchain_core.tools import BaseTool, tool\n",
    "@tool\n",
    "def get_cancer_context(docs):\n",
    "    \"\"\"\n",
    "    Tool to extract the relevant clinial context from the retrieved documents. \n",
    "    This tool will be how we prepare the reasoning for the agent, to ensure \n",
    "    correct hybrid results for the NG12 Cancer Risk Assessor guidelines engine.\n",
    "    \"\"\"\n",
    "\n",
    "    context =[]\n",
    "    for i, doc in enumerate(docs):\n",
    "        page = doc.metadata.get(\"page\", \"Unknown Page\")\n",
    "        section = doc.metadata.get(\"section\", \"General Guidance\")\n",
    "\n",
    "        block = (\n",
    "            f\"[CLINICAL EVIDENCEM {i+1}]\\n\"\n",
    "            f\"SOURCE: NICE Guideline NG12, Page {page}, Section: {section}\\n\"\n",
    "            f\"TEXT: {doc.page_content}\\n\"\n",
    "        )\n",
    "        context.append(block)\n",
    "    return \"\\n---\\n\".join(context)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_stable312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
