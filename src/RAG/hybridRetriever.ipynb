{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b00de5a0",
   "metadata": {},
   "source": [
    "# Safely Store ENV variables for Gemini, Chroma DB connection, and vector embeddings\n",
    "### Additionl techniques: cleared memory of any existing variables to avoid accidental exposure. Performed due to significant security risk of exposing PII and PHI for healthcare data.\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b43dd47f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection successful\n"
     ]
    }
   ],
   "source": [
    "# # load dotenv + imports for retriever tool\n",
    "from getpass import getpass\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_chroma import Chroma \n",
    "from langchain.tools import tool\n",
    "import langchainhub as hub\n",
    "import chromadb\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "\n",
    "api_key = os.getenv(\"CHROMA_API_KEY\") or getpass(\"Paste CHROMA_API_KEY: \")\n",
    "tenant = os.getenv(\"CHROMA_TENANT\") or input(\"Enter CHROMA_TENANT: \")\n",
    "database = os.getenv(\"CHROMA_DATABASE\") or input(\"Enter CHROMA_DATABASE: \")\n",
    "gemini_key = os.getenv(\"GEMINI_API_KEY\") or getpass(\"Enter GEMINI_API_KEY: \")\n",
    "\n",
    "# Load environment variables from chromaDB --CloudClient\n",
    "client = chromadb.CloudClient(\n",
    "    api_key=api_key,\n",
    "    tenant=tenant,\n",
    "    database=database\n",
    ")\n",
    "collection = client.get_collection(name=\"nice_org\")\n",
    "\n",
    "# load env to variable for langchain\n",
    "os.environ[\"GOOGLE_API_KEY\"] = gemini_key\n",
    "# connecting embeddings with Gemini\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/text-embedding-004\")\n",
    "\n",
    "vectorstore = Chroma(\n",
    "    client=client,\n",
    "    collection_name=\"nice_org\",\n",
    "    embedding_function=embeddings\n",
    ")\n",
    "\n",
    "print(\"Connection successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0797fb8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/briannamitchell/miniconda3/envs/rag_stable312/lib/python3.12/pty.py:95: DeprecationWarning: This process (pid=7152) is multi-threaded, use of forkpty() may lead to deadlocks in the child.\n",
      "  pid, fd = os.forkpty()\n"
     ]
    }
   ],
   "source": [
    "# ! pip install -qU langchain langchain-core langchain-community\n",
    "#! pip install -U langchain langchain-community langchain-chroma rank_bm25 flashrank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c67eafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import retriever tool BM25 for hybrid search\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "from langchain_core.retrievers import BaseRetriever\n",
    "from langchain_classic.retrievers import EnsembleRetriever # have to import classic for hyrbid ensemble retriever "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871b5c7a",
   "metadata": {},
   "source": [
    "# Hybrid Retrieval Search with BM25 semntic + keyword search\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "091aba63",
   "metadata": {},
   "source": [
    "### Semantic Retriever\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2e377a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# semantics\n",
    "vector_retriever = vectorstore.as_retriever(\n",
    "    search_kwargs={\"k\": 5}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ffebad",
   "metadata": {},
   "source": [
    "###  Lexical Retriever with BM25\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a4e86e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fbad987c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import documents for BM25 retriever\n",
    "raw_data = vectorstore.get()\n",
    "\n",
    "#pulling documents from chromaDB collection and the metadata to ensure the agent is able to cite and pull relevant info in answers\n",
    "documents_for_BM25 = [\n",
    "    Document(page_content=text, metatdata=meta)\n",
    "    for text, meta in zip(raw_data[\"documents\"], raw_data[\"metadatas\"])\n",
    "]\n",
    "documents = collection.get(include=[\"documents\"])[\"documents\"]\n",
    "\n",
    "\n",
    "# bm25 retriever for keyword search\n",
    "bm25_retriever = BM25Retriever.from_documents(documents_for_BM25)\n",
    "bm25_retriever.k = 20 # set k to 20 to ensure no matches are missed; using 5 for reranker instead\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89405f2",
   "metadata": {},
   "source": [
    "### Combine retrievers with EnsembleRetriever + BM25 for hybrid retrieval \n",
    "---\n",
    "### **Tradeoffs**: \n",
    "\n",
    "#### **Balance vs. Imbalanced Weights for retrievers:**\n",
    "\n",
    "**Why did I use 0.5 semantic and 0.5 lexical for the weights in my hybrid retriever?**\n",
    "\n",
    "**Reasoning:** \n",
    "I wanted to balance the weight + importance of both the semantic and lexical retrieval meaning. The way the query's are formatted is unknown to me at this stage, so the best bet was to start with an equal balance and then adjust as needed.\n",
    "\n",
    "Also, to compensate for the equal balance, I'm adding a lightweight **re-ranker LLM FlashRank**, it uses a pointwise reranking system. I chose this because it's fast, efficient, and runs FREE locally on my machine. At the end of the day, I would say that the semantic retriever will likely be more important for the types of queries that will be used in my clinical application, but I wanted to give the lexical retriever a fair chance to contribute as well.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3678520e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combinination of retrievers\n",
    "hybrid_retriever = EnsembleRetriever(\n",
    "    retrievers=[vector_retriever, bm25_retriever],\n",
    "    weights=[0.5, 0.5]\n",
    ")\n",
    "\n",
    "results = hybrid_retriever.get_relevant_documents(\"What symptoms trigger an urgent referral for mastitis?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbbfd8a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_stable312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
