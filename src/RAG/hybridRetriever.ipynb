{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b00de5a0",
   "metadata": {},
   "source": [
    "# Safely Store ENV variables for Gemini, Chroma DB connection, and vector embeddings\n",
    "### Additional techniques: cleared memory of any existing variables to avoid accidental exposure. Performed due to significant security risk of exposing PII and PHI for healthcare data.\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "4be341c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSTALLS\n",
    "#! pip install rank-bm25\n",
    "#! pip install langchain-classic rank_bm25 langchain-community\n",
    "#! pip install -U langchain-google-vertexai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58778aef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/briannamitchell/miniconda3/envs/rag_stable312/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Disabling PyTorch because PyTorch >= 2.4 is required but found 2.2.2\n",
      "PyTorch was not found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    }
   ],
   "source": [
    "# imports \n",
    "# # load dotenv + imports for retriever tool\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_chroma import Chroma \n",
    "from langchain.tools import tool\n",
    "import langchainhub as hub\n",
    "import chromadb\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings #type:ignore\n",
    "import vertexai\n",
    "from vertexai.language_models import TextEmbeddingModel\n",
    "# from langchain_community.retrievers import BM25Retriever\n",
    "# from langchain_core.runnables import RunnableLambda, RunnableSequence, RunnableMap, RunnablePassthrough, RunnableParallel\n",
    "from langchain_core.runnables import Runnable\n",
    "# from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "# from llama_index.core import StorageContext, VectorStoreIndex, Document\n",
    "# from llama_index.retrievers.bm25 import BM25Retriever\n",
    "# from llama_index.core.retrievers import QueryFusionRetriever\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b43dd47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os \n",
    "# Load environment variables from .env file first\n",
    "load_dotenv()\n",
    "\n",
    "#api_key = os.getenv(\"CHROMA_API_KEY\") or getpass(\"Paste CHROMA_API_KEY: \")\n",
    "#tenant = os.getenv(\"CHROMA_TENANT\") or input(\"Enter CHROMA_TENANT: \")\n",
    "#database = os.getenv(\"CHROMA_DATABASE\") or input(\"Enter CHROMA_DATABASE: \")\n",
    "#gemini_key = os.getenv(\"GEMINI_API_KEY\") or getpass(\"Enter GEMINI_API_KEY: \")\n",
    "#vertex_ai_key = os.getenv(\"GOOGLE_APPLICATION_CREDENTIALS\") or getpass(\"Enter GOOGLE_APPLICATION_CREDENTIALS: \")\n",
    "#openAI_api_key = os.getenv(\"OPENAI_API_KEY\") or getpass(\"Enter OPENAI_API_KEY\")\n",
    "\n",
    "os.environ[\"GOOGLE_API_KEY\"] = os.getenv(\"GEMINI_API_KEY\", \"\")\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = os.getenv(\"GOOGLE_APPLICATION_CREDENTIALS\", \"\")\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\", \"\")\n",
    "\n",
    "# Load environment variables from chromaDB with persistence locally, and prevent unecessary calls to API--more cost-effective approach\n",
    "persistent_client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "\n",
    "# Ensure the GEMINI_API_KEY environment variable is set\n",
    "gemini_api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "if not gemini_api_key:\n",
    "\tfrom getpass import getpass\n",
    "\tgemini_api_key = getpass(\"Enter GEMINI_API_KEY: \")\n",
    "\tos.environ[\"GEMINI_API_KEY\"] = gemini_api_key\n",
    "\n",
    "# Importing Gemini for embedding\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"gemini-embedding-001\", api_key=gemini_api_key)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53e2be3",
   "metadata": {},
   "source": [
    "# **Challenges + Mitigation strategies for Performing Vector Search:**\n",
    "---\n",
    "#### **Tradeoffs:**\n",
    "- Originally, my plan was to use the same framework for both retrievers, then fetch the data from my vector store in chroma. However, I found that my BM25 retriever simply would not return any data for either framework--langchain or llamaindex. Later on, I found out that due to how the BM25-Retriever architecture is built, the keyword search library needs access to pull data from **raw text nodes**. This does not align with my current Chroma vector store DB architecture, which is built **with stored embeddings--not original text**. Hence, why orginally no data was being returned in my prior iteration. \n",
    "\n",
    "#### **How I solved for this/Why I chose this method?:**\n",
    "To mitigate for this, I decided to change my framework and use llamaindex for BM25/lexical retriever; it provides a much more seamless integration with the Chroma vector store architecture. Then, I used llamaindex to pass my list of nodes directly to the BM25 retriever, which solved the short-term problem.\n",
    "\n",
    "#### **Improvement with Llama_Index:**\n",
    "- Use raw text nodes for BM25 retrieval to ensure compatibility with the keyword search library\n",
    "- Implement response enhancement by providing additional context or *page_content* for my document queries.\n",
    "\n",
    "---\n",
    "\n",
    "## **Pitfall/Tradeoffs with my original approach:**\n",
    "While attempting to test the combination of langchain and llamaIndex for different retrievers, I came across multiple problems. The main issue, my database and embeddings are in ChromaDB. The issue arises when performing a hybrid retrieval--where you **combine both a semantic, dense retriever and a lexical, keyword precise retriever**. The **root problem stemmed from BM25's architecture**. Semantic retriever seamlessy integrates with my dB because it uses the **vector_store from Chroma**. My BM25 does not integrate well because it **must pass the queried Document objects as nodes**--**something that can only be performed with VectorStoreIndex in LlamaIndex**. Being that Chroma is very langchain dependent and VectorStoreIndex is LlamaIndex dependant, combining these two very different methods would result in uneccessary overhead. I also had to factor in the limited constraint of having to use Gemini 1.5 with VertexAI embeddings, which is a closed-solution.\n",
    "\n",
    "- If I had to start over, to successfully integrate a hybrid retrieval RAG, I would have used either **OpenAI embeddings, ChatGPT4o, and ChromaDB** to still keep the closed-source solution in production. \n",
    "\n",
    "- If building proof of concept locally, I would use **FAISS for my vector store** + replace my **model with Gemma from HuggingFace.** for a more open source solution.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45313c8",
   "metadata": {},
   "source": [
    "# **Final RAG Approach:**\n",
    "---\n",
    "\n",
    "## My main consideration...**How can I pivot without losing retrieval quality?****\n",
    "My plan is to move to a more powerful approach by performing accurate similarity search with my **vector_store in ChromaDb** as my retriever in langchain. This is done to obtain exact matches with the potential of ingesting large documents via **Approximate Nearest Neighbors (ANN) algorithm built-in to ChromaDB**. Then, I will apply **Maximum Marginal Relevance(MMR)**. It's a great pivot from my prior solution. I'm able to optimize for balance between **relevance and diversity in a faster time complexity**. In a live healthcare environment, my agent would come in contact with multiple provider oppinions. In order to accurately generate responses that mimic live production, I would need to compensate for scaling this factor later on. Hence, whyn I chose this method.\n",
    "**Benefits:**\n",
    "- Prioritizes relevance and diversity during retrieval\n",
    "- Compensates for random spikes during data ingestion\n",
    "- Balances exploration with exploitation\n",
    "- Ensures my retrieved documents are distinct from each other"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89fb84c8",
   "metadata": {},
   "source": [
    "# **Creating VectorStore for LangChain + Enhancing Responses by Verifying Context Retrieved: How I Improved Generator Output:** \n",
    "---\n",
    "### Used response enhancement as a guideline for the model to follow when verifying retrieved content, building a better **knowledge base engine** for my agent to fetch. This system design technique results in a more enhanced **RAG performance + quality**. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29ba2418",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/36/76yfxjrs77l1ss7r47fqfykr0000gn/T/ipykernel_1330/1904139494.py:11: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the `langchain-chroma package and should be used instead. To use it run `pip install -U `langchain-chroma` and import as `from `langchain_chroma import Chroma``.\n",
      "  vectorstore = Chroma(\n"
     ]
    }
   ],
   "source": [
    "# adding response enhancement for generated outputs\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "#get collection name from chromaDB\n",
    "#COLLECTION_NAME = \"ng12\"\n",
    "#chroma_collection = client.get_collection(name=COLLECTION_NAME)\n",
    "#embed_model = embeddings\n",
    "\n",
    "#vectore store\n",
    "vectorstore = Chroma(\n",
    "    collection_name=\"ng12\",\n",
    "    embedding_function=embeddings,\n",
    "    client=persistent_client\n",
    ")\n",
    "\n",
    "#storage context\n",
    "#storage_context = StorageContext.from_defaults(vectorstore=vectorstore)\n",
    "#storage_context.persist()\n",
    "\n",
    "documents = [\n",
    "    Document(page_content=\"Shortness of breath with cough or fatigue or chest pain or weight loss or appetite loss (unexplained), 40 and over: possible cancer Lung or mesothelioma\", metadata={\"referral\": \"Urgent\", \"source\": \"Suspected cancer: recognition and referral (NG12) 2026\", \"page\": \"52\"}),\n",
    "    Document(page_content=\"Bleeding, bruising or petechiae, unexplained: possible cancer Leukaemia\", metadata={\"referral\": \"Very urgent\", \"source\": \"Suspected cancer: recognition and referral (NG12) 2026\", \"page\": \"43\"}),\n",
    "    Document(page_content=\"Fracture unexplained, 60 and over: possible cancer Myeloma\", metadata={\"referral\": \"Unexplained\", \"source\": \"Suspected cancer: recognition and referral (NG12) 2026\", \"page\": \"55\"}),\n",
    "    Document(page_content=\"Refer people using a suspected cancer pathway referral for oesophageal cancer if they: have dysphagia or, are aged 55 and over, with weight loss, and they have any of the following: upper abdominal pain, reflux, dyspepsia. [2015, amended 2025]\", metadata={\"referral\": \"Suspected cancer pathway referral\", \"source\": \"Suspected cancer: recognition and referral (NG12) 2026\", \"page\": \"11\"}),\n",
    "    Document(page_content=\"Skin lesion that raises the suspicion of a basal cell carcinoma: possible cancer Basal cell carcinoma  \", metadata={\"referral\": \"Raises the suspicion of\", \"source\": \"Suspected cancer: recognition and referral (NG12) 2026\", \"page\": \"58\"}),\n",
    "    Document(page_content=\"Urinary urgency or frequency, increased and persistent or frequent, particularly more than 12 times per month in women, especially if 50 and over: possible cancer Ovarian\", metadata={\"referral\": \"Persistent\", \"source\": \"Suspected cancer: recognition and referral (NG12) 2026\", \"page\": \"60\"}),\n",
    "    Document(page_content=\"Upper abdominal pain with low haemoglobin levels or raised platelet count or nausea or vomiting, 55 and over: possible cancer Oesophageal or stomach \", metadata={\"referral\": \"Non-urgent\", \"source\": \"Suspected cancer: recognition and referral (NG12) 2026\", \"page\": \"40\"}),\n",
    "    Document(page_content=\"Petechiae unexplained in children and young people: possible cancer Leukaemia\", metadata={\"referral\": \"Immediate\", \"source\": \"Suspected cancer: recognition and referral (NG12) 2026\", \"page\": \"74\"})\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63265b9f",
   "metadata": {},
   "source": [
    "##### Maximum Marginal Relevance:\n",
    "---\n",
    "I use my lambda (ƛ) value as a threshold:\n",
    "- 0 = Max diversity ƛ=0\n",
    "- 1 = max relevance ƛ=1\n",
    "\n",
    "**Potential Tuning for MMR:**\n",
    "- Navigational/Exact--> ƛ= 0.7 - 0.9: \"Is this patient at 403 an urgent referral?\"\n",
    "- Balanced/Research--> ƛ= 0.5 - 0.7: \"What is the best referral recommendation based on symptoms of patient403?\"\n",
    "- Exploratory/Diverse--> ƛ= 0.3 - 0.5: \"Give me a summary of findings and determine referral status..\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8c5813a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create retriever from vector store and then apply MMR \n",
    "\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"mmr\",\n",
    "    search_kwargs={\n",
    "        \"k\": 8,\n",
    "        \"fetch_k\": 50,\n",
    "        \"lambda_mult\": 0.6\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2e741d",
   "metadata": {},
   "source": [
    "# Techniques used(for my readme):\n",
    "- vector store as retrieval\n",
    "- response enhancement\n",
    "- MMR to diversify the output response \n",
    "- Source attribution--> helps prevent hallucinations in the model to output where information comes from\n",
    "- add Maximal marginal relevance to ensure a balance of diversity and relevance retrieved"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc721263",
   "metadata": {},
   "source": [
    "# **Perform Context Engineering**\n",
    "---\n",
    "### Tradeoffs:\n",
    "\n",
    "**Why did I choose this approach for context engineering?**\n",
    "\n",
    "**Reasoning:** \n",
    "At first, my goal was to return a single string of relevant documents. However, I realized that my agent would have trouble parsing my actual data in production. Based on the way the data is formatted in retrieval, I had to make some adjustments, and go a bit deeper into context engineering. Also, one of the key constraints in building this agent included a task to cite the specific sources within the relevant data retrieved from the NG12 documents. I could have kept my single string, however, there would have been significant fine-tuning and overhead later. Performing context engineering was the only plausible to way achieve this goal. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc77bd73",
   "metadata": {},
   "source": [
    "#### Adding Clinical Context Tool\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6704c5f4",
   "metadata": {},
   "source": [
    "# **STOPPING POINT BEFORE ADDING SOURCE ATTRIBUTION TO GET BETTER CONTEXT:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b41a4d5",
   "metadata": {},
   "source": [
    "# Source attribution Prompt\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ae7e25d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# attrbution\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "attribution_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\" You are a precise and accurate clinical AI assistant that provides expert knowledge in\n",
    "    clinical decision-making support for those who encounter direct patient care.\n",
    "    You have inherited a persona profile module as your agentic architecture.\n",
    "    Main Task: Your main task is to determine whether the presented patient requires an urgent referral or not.\n",
    "    Use the information provided from the text corpus below:\n",
    "    The National Institute for Health and Care Excellence (NICE) Guideline for Suspected cancer: recognition and referral NICE guideline.\n",
    "    Once, main task is complete and accurate, you must provide a recommendation that decides post-referral instructions corresponding to the most relevant medical imaging practices.\n",
    "    If patient does not meet urgent referral criteria, do not recommend any medical imaging practices.\n",
    "\n",
    "    Answer the following question based ONLY on the provided sources. \n",
    "    For each fact or claim in your answer include a citation that refers to the source.\n",
    "\n",
    "    Do not make up information or provide personal opinions in your responses without verifying answers with evidence.\n",
    "    You must cite the specific sources you found from the NICE guidelines in this specific format below:\n",
    "    This is how you are expected to format: [referral type: insert referral, source: name of text corpus, (year published), page: insert page number]\n",
    "    \n",
    "    Your source attributes at the end of your responses will look like this template below in practice:\n",
    "    [referral: Persistent, source: Suspected cancer: recognition and referral (NG12), 2026, page: 60]\n",
    "\n",
    "    How your input and output will be formatted with citation sources used:\n",
    "    Question: {question}\n",
    "\n",
    "    Sources: {sources}\n",
    "\n",
    "    Your answer: {answer}\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e90c03",
   "metadata": {},
   "source": [
    "### Helper functions to further format the sources with citation numbers and generate attributed responses\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7b7169c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#source formatted strings from documents\n",
    "def format_sources_with_citations(documents):\n",
    "    formatted_sources = []\n",
    "    for i, doc in enumerate(documents, 1):\n",
    "        source_info = f\"[{i} {doc.metadata.get('source', 'Unknown source')}]\"\n",
    "        if doc.metadata.get('page'):\n",
    "            source_info += f\", page {doc.metadata['page']}\"\n",
    "        formatted_sources.append(f\"{source_info}\\n{doc.page_content}\")\n",
    "    return \"\\n\\n\".join(formatted_sources)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858fc502",
   "metadata": {},
   "source": [
    "### Building RAG chain with my source attribution \n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8acd626",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unexpected argument 'vertex_ai_location' provided to ChatGoogleGenerativeAI. Did you mean: 'vertexai'?\n",
      "/Users/briannamitchell/miniconda3/envs/rag_stable312/lib/python3.12/site-packages/IPython/core/interactiveshell.py:3701: UserWarning: WARNING! vertex_ai_location is not default parameter.\n",
      "                vertex_ai_location was transferred to model_kwargs.\n",
      "                Please confirm that vertex_ai_location is what you intended.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    },
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for ChatGoogleGenerativeAI\nclient\n  Input should be an instance of Client [type=is_instance_of, input_value=<vertexai._genai.client.C...t object at 0x1208e4140>, input_type=Client]\n    For further information visit https://errors.pydantic.dev/2.12/v/is_instance_of",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValidationError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[38]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# invoking my retriever for my relevant docs\u001b[39;00m\n\u001b[32m      3\u001b[39m client=vertexai.Client(project=\u001b[33m\"\u001b[39m\u001b[33mCancerRiskAgent\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m model = \u001b[43mChatGoogleGenerativeAI\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgemini-1.5\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgoogle_api_key\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgemini_api_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclient\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvertex_ai_location\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mus-east4\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m     11\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerated_attributed_response\u001b[39m(question):\n\u001b[32m     14\u001b[39m     retrieved_docs = retriever.invoke(\u001b[33m\"\u001b[39m\u001b[33mquestion\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/rag_stable312/lib/python3.12/site-packages/langchain_google_genai/chat_models.py:2269\u001b[39m, in \u001b[36mChatGoogleGenerativeAI.__init__\u001b[39m\u001b[34m(self, **kwargs)\u001b[39m\n\u001b[32m   2262\u001b[39m         suggestion = (\n\u001b[32m   2263\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m Did you mean: \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msuggestions[\u001b[32m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m?\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m suggestions \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2264\u001b[39m         )\n\u001b[32m   2265\u001b[39m         logger.warning(\n\u001b[32m   2266\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnexpected argument \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00marg\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2267\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mprovided to ChatGoogleGenerativeAI.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msuggestion\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   2268\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m2269\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/rag_stable312/lib/python3.12/site-packages/langchain_core/load/serializable.py:118\u001b[39m, in \u001b[36mSerializable.__init__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args: Any, **kwargs: Any) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    117\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\"\"\"\u001b[39;00m  \u001b[38;5;66;03m# noqa: D419  # Intentional blank docstring\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m118\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/rag_stable312/lib/python3.12/site-packages/pydantic/main.py:250\u001b[39m, in \u001b[36mBaseModel.__init__\u001b[39m\u001b[34m(self, **data)\u001b[39m\n\u001b[32m    248\u001b[39m \u001b[38;5;66;03m# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\u001b[39;00m\n\u001b[32m    249\u001b[39m __tracebackhide__ = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m validated_self = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__pydantic_validator__\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalidate_python\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mself_instance\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m validated_self:\n\u001b[32m    252\u001b[39m     warnings.warn(\n\u001b[32m    253\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mA custom validator is returning a value other than `self`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m'\u001b[39m\n\u001b[32m    254\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mReturning anything other than `self` from a top level model validator isn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt supported when validating via `__init__`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    255\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mSee the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    256\u001b[39m         stacklevel=\u001b[32m2\u001b[39m,\n\u001b[32m    257\u001b[39m     )\n",
      "\u001b[31mValidationError\u001b[39m: 1 validation error for ChatGoogleGenerativeAI\nclient\n  Input should be an instance of Client [type=is_instance_of, input_value=<vertexai._genai.client.C...t object at 0x1208e4140>, input_type=Client]\n    For further information visit https://errors.pydantic.dev/2.12/v/is_instance_of"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "# invoking my retriever for my relevant docs\n",
    "client=vertexai.Client(vertexai=True, project=\"CancerRiskAgent\", location=\"us-east4\")\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-pro\",\n",
    "    temperature=0.0,\n",
    "    client=client,\n",
    ")\n",
    "\n",
    "def generated_attributed_response(question: str):\n",
    "    retrieved_docs = retriever.invoke(question)\n",
    "    sources_formatted = format_sources_with_citations(retrieved_docs)\n",
    "\n",
    "    #create attribution chain, I'll use LCEL methods\n",
    "    attribution_chain = (\n",
    "        attribution_prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    response = attribution_chain.invoke({\n",
    "        \"question\": question,\n",
    "        \"sources\": sources_formatted\n",
    "    })\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "eeca99a9",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for ChatGoogleGenerativeAI\nmodel\n  Input should be a valid string [type=string_type, input_value=ChatGoogleGenerativeAI(pr...ata=(), model_kwargs={}), input_type=ChatGoogleGenerativeAI]\n    For further information visit https://errors.pydantic.dev/2.12/v/string_type",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValidationError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[36]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m question = \u001b[33m\"\u001b[39m\u001b[33mWhat are some symptoms that require an urgent referral?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m answer = \u001b[43mgenerated_attributed_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[35]\u001b[39m\u001b[32m, line 13\u001b[39m, in \u001b[36mgenerated_attributed_response\u001b[39m\u001b[34m(question)\u001b[39m\n\u001b[32m      8\u001b[39m sources_formatted = format_sources_with_citations(retrieved_docs)\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m#create attribution chain, I'll use LCEL methods\u001b[39;00m\n\u001b[32m     11\u001b[39m attribution_chain = (\n\u001b[32m     12\u001b[39m     attribution_prompt\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m     | \u001b[43mChatGoogleGenerativeAI\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m \n\u001b[32m     14\u001b[39m     | StrOutputParser()\n\u001b[32m     15\u001b[39m )\n\u001b[32m     16\u001b[39m response = attribution_chain.invoke({\n\u001b[32m     17\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mquestion\u001b[39m\u001b[33m\"\u001b[39m: question,\n\u001b[32m     18\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33msources\u001b[39m\u001b[33m\"\u001b[39m: sources_formatted\n\u001b[32m     19\u001b[39m })\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/rag_stable312/lib/python3.12/site-packages/langchain_google_genai/chat_models.py:2269\u001b[39m, in \u001b[36mChatGoogleGenerativeAI.__init__\u001b[39m\u001b[34m(self, **kwargs)\u001b[39m\n\u001b[32m   2262\u001b[39m         suggestion = (\n\u001b[32m   2263\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m Did you mean: \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msuggestions[\u001b[32m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m?\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m suggestions \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2264\u001b[39m         )\n\u001b[32m   2265\u001b[39m         logger.warning(\n\u001b[32m   2266\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnexpected argument \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00marg\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2267\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mprovided to ChatGoogleGenerativeAI.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msuggestion\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   2268\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m2269\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/rag_stable312/lib/python3.12/site-packages/langchain_core/load/serializable.py:118\u001b[39m, in \u001b[36mSerializable.__init__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args: Any, **kwargs: Any) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    117\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\"\"\"\u001b[39;00m  \u001b[38;5;66;03m# noqa: D419  # Intentional blank docstring\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m118\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/rag_stable312/lib/python3.12/site-packages/pydantic/main.py:250\u001b[39m, in \u001b[36mBaseModel.__init__\u001b[39m\u001b[34m(self, **data)\u001b[39m\n\u001b[32m    248\u001b[39m \u001b[38;5;66;03m# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\u001b[39;00m\n\u001b[32m    249\u001b[39m __tracebackhide__ = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m validated_self = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__pydantic_validator__\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalidate_python\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mself_instance\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m validated_self:\n\u001b[32m    252\u001b[39m     warnings.warn(\n\u001b[32m    253\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mA custom validator is returning a value other than `self`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m'\u001b[39m\n\u001b[32m    254\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mReturning anything other than `self` from a top level model validator isn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt supported when validating via `__init__`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    255\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mSee the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    256\u001b[39m         stacklevel=\u001b[32m2\u001b[39m,\n\u001b[32m    257\u001b[39m     )\n",
      "\u001b[31mValidationError\u001b[39m: 1 validation error for ChatGoogleGenerativeAI\nmodel\n  Input should be a valid string [type=string_type, input_value=ChatGoogleGenerativeAI(pr...ata=(), model_kwargs={}), input_type=ChatGoogleGenerativeAI]\n    For further information visit https://errors.pydantic.dev/2.12/v/string_type"
     ]
    }
   ],
   "source": [
    "question = \"What are some symptoms that require an urgent referral?\"\n",
    "answer = generated_attributed_response(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763b2efb",
   "metadata": {},
   "source": [
    "### Consistency Checking for Accuracy\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_stable312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
