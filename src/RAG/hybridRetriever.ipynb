{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b00de5a0",
   "metadata": {},
   "source": [
    "# Safely Store ENV variables for Gemini, Chroma DB connection, and vector embeddings\n",
    "### Additionl techniques: cleared memory of any existing variables to avoid accidental exposure. Performed due to significant security risk of exposing PII and PHI for healthcare data.\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43dd47f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gemini embedding dimensions: 3072\n",
      "Found 300 chunks in 'ng12'.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 3, got 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 77\u001b[39m\n\u001b[32m     71\u001b[39m new_data = new_vs.get(include=[\u001b[33m\"\u001b[39m\u001b[33mdocuments\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmetadatas\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     72\u001b[39m documents_from_chroma = [\n\u001b[32m     73\u001b[39m     Document(page_content=text, metadata=(meta \u001b[38;5;129;01mor\u001b[39;00m {}))\n\u001b[32m     74\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m text, meta \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(new_data[\u001b[33m\"\u001b[39m\u001b[33mdocuments\u001b[39m\u001b[33m\"\u001b[39m], new_data[\u001b[33m\"\u001b[39m\u001b[33mmetadatas\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     75\u001b[39m ]\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m bm25_retriever = \u001b[43mBM25Retriever\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocuments_from_chroma\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     78\u001b[39m bm25_retriever.k = \u001b[32m20\u001b[39m\n\u001b[32m     81\u001b[39m \u001b[38;5;66;03m#hybrid retriever \u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/rag_stable312/lib/python3.12/site-packages/langchain_community/retrievers/bm25.py:99\u001b[39m, in \u001b[36mBM25Retriever.from_documents\u001b[39m\u001b[34m(cls, documents, bm25_params, preprocess_func, **kwargs)\u001b[39m\n\u001b[32m     79\u001b[39m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[32m     80\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfrom_documents\u001b[39m(\n\u001b[32m     81\u001b[39m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     86\u001b[39m     **kwargs: Any,\n\u001b[32m     87\u001b[39m ) -> BM25Retriever:\n\u001b[32m     88\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     89\u001b[39m \u001b[33;03m    Create a BM25Retriever from a list of Documents.\u001b[39;00m\n\u001b[32m     90\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     97\u001b[39m \u001b[33;03m        A BM25Retriever instance.\u001b[39;00m\n\u001b[32m     98\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m99\u001b[39m     texts, metadatas, ids = \u001b[38;5;28mzip\u001b[39m(\n\u001b[32m    100\u001b[39m         *((d.page_content, d.metadata, d.id) \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m documents)\n\u001b[32m    101\u001b[39m     )\n\u001b[32m    102\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m.from_texts(\n\u001b[32m    103\u001b[39m         texts=texts,\n\u001b[32m    104\u001b[39m         bm25_params=bm25_params,\n\u001b[32m   (...)\u001b[39m\u001b[32m    108\u001b[39m         **kwargs,\n\u001b[32m    109\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: not enough values to unpack (expected 3, got 0)"
     ]
    }
   ],
   "source": [
    "# # load dotenv + imports for retriever tool\n",
    "from getpass import getpass\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_chroma import Chroma \n",
    "from langchain.tools import tool\n",
    "import langchainhub as hub\n",
    "import chromadb\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "from langchain_classic.retrievers.ensemble import EnsembleRetriever\n",
    "from langchain_core.runnables import RunnableLambda, RunnableSequence, RunnableMap, RunnablePassthrough, RunnableParallel\n",
    "from langchain_core.runnables import Runnable\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "api_key = os.getenv(\"CHROMA_API_KEY\") or getpass(\"Paste CHROMA_API_KEY: \")\n",
    "tenant = os.getenv(\"CHROMA_TENANT\") or input(\"Enter CHROMA_TENANT: \")\n",
    "database = os.getenv(\"CHROMA_DATABASE\") or input(\"Enter CHROMA_DATABASE: \")\n",
    "gemini_key = os.getenv(\"GEMINI_API_KEY\") or getpass(\"Enter GEMINI_API_KEY: \")\n",
    "\n",
    "os.environ[\"GOOGLE_API_KEY\"] = gemini_key\n",
    "\n",
    "# Load environment variables from chromaDB --CloudClient\n",
    "client = chromadb.CloudClient(\n",
    "    api_key=api_key, \n",
    "    tenant=tenant, \n",
    "    database=database)\n",
    "\n",
    "OLD_COLLECTION = \"ng12\"\n",
    "NEW_COLLECTION = \"NG12_GEMINI3072\"\n",
    "\n",
    "embedding = GoogleGenerativeAIEmbeddings(model=\"models/gemini-embedding-001\")\n",
    "\n",
    "#CHECK DIMENSIONS\n",
    "vec = embedding.embed_query(\"dimension check\")\n",
    "print(\"Gemini embedding dimensions:\", len(vec))\n",
    "\n",
    "# attaching old to new collection with embedding function for semantic search\n",
    "old_vs = Chroma(\n",
    "    client=client,\n",
    "    collection_name=OLD_COLLECTION\n",
    ")\n",
    "\n",
    "old_data = old_vs.get(include=[\"documents\", \"metadatas\"])\n",
    "old_docs = old_data.get(\"documents\", [])\n",
    "old_metas = old_data.get(\"metadatas\", [])\n",
    "\n",
    "if not old_docs:\n",
    "    raise ValueError(f\"No documents found in OLD collection '{OLD_COLLECTION}'.\")\n",
    "\n",
    "docs_to_copy = [\n",
    "    Document(page_content=text, metadata=(meta or {}))\n",
    "    for text, meta in zip(old_docs, old_metas)\n",
    "]\n",
    "\n",
    "print(f\"Found {len(docs_to_copy)} chunks in '{OLD_COLLECTION}'.\")\n",
    "\n",
    "#new colletion with GEMINI\n",
    "new_vs = Chroma(\n",
    "    client=client,\n",
    "    collection_name=NEW_COLLECTION,\n",
    "    embedding_function=embedding\n",
    ")\n",
    "\n",
    "#semantic retriever/ vector retriever\n",
    "vector_retriever = new_vs.as_retriever(search_kwargs={\"k\": 5})\n",
    "\n",
    "# keyword with BM25 retriever\n",
    "new_data = new_vs.get(include=[\"documents\", \"metadatas\"])\n",
    "documents_from_chroma = [\n",
    "    Document(page_content=text, metadata=(meta or {}))\n",
    "    for text, meta in zip(new_data[\"documents\"], new_data[\"metadatas\"])\n",
    "]\n",
    "\n",
    "bm25_retriever = BM25Retriever.from_documents(documents_from_chroma)\n",
    "bm25_retriever.k = 20\n",
    "\n",
    "\n",
    "#hybrid retriever \n",
    "hybrid_retriever = EnsembleRetriever(\n",
    "    retrievers=[vector_retriever, bm25_retriever],\n",
    "    weights=[0.5, 0.5]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e9cdacb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding class: <class 'langchain_google_genai.embeddings.GoogleGenerativeAIEmbeddings'>\n",
      "embedding dim: 3072\n"
     ]
    }
   ],
   "source": [
    "# 1) What does YOUR embedding object output?\n",
    "vec = embedding.embed_query(\"test\")\n",
    "print(\"embedding class:\", type(embedding))\n",
    "print(\"embedding dim:\", len(vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0c8c93b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vectorstore embedding: <class 'langchain_google_genai.embeddings.GoogleGenerativeAIEmbeddings'>\n",
      "vectorstore embedding dim: 3072\n"
     ]
    }
   ],
   "source": [
    "print(\"vectorstore embedding:\", type(vectorstore._embedding_function))\n",
    "test_vec2 = vectorstore._embedding_function.embed_query(\"test\")\n",
    "print(\"vectorstore embedding dim:\", len(test_vec2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd3fbc2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper for context engineering\n",
    "def get_cancer_context(docs):\n",
    "    context = []\n",
    "    for i, doc in enumerate(docs):\n",
    "        page = doc.metatdata.get(\"page\", \"?\")\n",
    "        block = f\"[Source: Page {page}] {doc.page_content}\"\n",
    "        context.append(block)\n",
    "    return \"\\n---\\n\".join(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0797fb8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install -qU langchain langchain-core langchain-community\n",
    "#! pip install -U langchain langchain-community langchain-chroma rank_bm25 flashrank"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871b5c7a",
   "metadata": {},
   "source": [
    "# Hybrid Retrieval Search with BM25 semantic + keyword search\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "091aba63",
   "metadata": {},
   "source": [
    "### Semantic Retriever\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ffebad",
   "metadata": {},
   "source": [
    "###  Lexical Retriever with BM25\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbad987c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "collection_data = vectorstore.get()\n",
    "\n",
    "documents_from_chroma = [\n",
    "    Document(page_content=doc, metadata=meta)\n",
    "    for doc, meta in zip(\n",
    "        collection_data[\"documents\"],\n",
    "        collection_data[\"metadatas\"]\n",
    "    )\n",
    "]\n",
    "\n",
    "bm25_retriever = BM25Retriever.from_documents(documents_from_chroma)\n",
    "bm25_retriever.k = 20\n",
    "\n",
    "hybrid_retriever = EnsembleRetriever(\n",
    "    retrievers=[vector_retriever, bm25_retriever],\n",
    "    weights=[0.5, 0.5]\n",
    ")\n",
    "\n",
    "results = hybrid_retriever.invoke(\n",
    "    \"Urgent referral criteria for lung cancer\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89405f2",
   "metadata": {},
   "source": [
    "### **Combine retrievers with EnsembleRetriever + BM25 for hybrid retrieval**\n",
    "---\n",
    "### **Tradeoffs**: \n",
    "\n",
    "#### **Balance vs. Imbalanced Weights for retrievers:**\n",
    "---\n",
    "\n",
    "**Why did I use 0.5 semantic and 0.5 lexical for the weights in my hybrid retriever?**\n",
    "\n",
    "**Reasoning:** \n",
    "I wanted to balance the weight + importance of both the semantic and lexical retrieval meaning. The way the query's are formatted is unknown to me at this stage, so the best bet was to start with an equal balance and then adjust as needed.\n",
    "\n",
    "Also, to compensate for the equal balance, I'm adding a lightweight **re-ranker LLM FlashRank**, it uses a pointwise reranking system. I chose this because it's fast, efficient, and runs FREE locally on my machine. At the end of the day, I would say that the semantic retriever will likely be more important for the types of queries that will be used in my clinical application, but I wanted to give the lexical retriever a fair chance to contribute as well.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454818bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# langchain depreacted the get relevant docs method for Ensemble Retrievers, updated method uses runnable utilities\n",
    "from langchain_core.runnables import RunnableLambda, RunnableSequence, RunnableMap, RunnablePassthrough, RunnableParallel\n",
    "from langchain_core.runnables import Runnable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc721263",
   "metadata": {},
   "source": [
    "# Perform Context Engineering to prepare for Agentic Reasoning\n",
    "---\n",
    "### Tradeoffs:\n",
    "\n",
    "**Why did I choose this approach for context engineering?**\n",
    "\n",
    "**Reasoning:** \n",
    "At first, my goal was to return a single string of relevant documents. However, I realized that my agent would have trouble parsing my actual data in production. Based on the way the data is formatted, I had to make some adjustments, and go a bit deeper with formatting. Also, one of the key constraints in building this agent included a task to cite the specific sources within the relevant data retrieved from the NG12 documents. I could have kept my single string, however, there would have been significant fine-tuning and overhead later. Performing context engineering was the only plausible to way achieve this goal. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc77bd73",
   "metadata": {},
   "source": [
    "#### Adding Clinical Context Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5cf3f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clinical context tooling for response -pg143\n",
    "from langchain_core.tools import BaseTool, tool\n",
    "@tool\n",
    "def get_cancer_context(docs):\n",
    "    \"\"\"\n",
    "    Tool to extract the relevant clinial context from the retrieved documents. \n",
    "    This tool will be how we prepare the reasoning for the agent, to ensure \n",
    "    correct hybrid results for the NG12 Cancer Risk Assessor guidelines engine.\n",
    "    \"\"\"\n",
    "\n",
    "    context =[]\n",
    "    for i, doc in enumerate(docs):\n",
    "        page = doc.metadata.get(\"page\", \"Unknown Page\")\n",
    "        section = doc.metadata.get(\"section\", \"General Guidance\")\n",
    "\n",
    "        block = (\n",
    "            f\"[CLINICAL EVIDENCEM {i+1}]\\n\"\n",
    "            f\"SOURCE: NICE Guideline NG12, Page {page}, Section: {section}\\n\"\n",
    "            f\"TEXT: {doc.page_content}\\n\"\n",
    "        )\n",
    "        context.append(block)\n",
    "    return \"\\n---\\n\".join(context)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_stable312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
