{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b00de5a0",
   "metadata": {},
   "source": [
    "# Safely Store ENV variables for Gemini, Chroma DB connection, and vector embeddings\n",
    "### Additional techniques: cleared memory of any existing variables to avoid accidental exposure. Performed due to significant security risk of exposing PII and PHI for healthcare data.\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be341c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSTALLS\n",
    "#! pip install rank-bm25\n",
    "#! pip install langchain-classic rank_bm25 langchain-community\n",
    "#! pip install -U langchain-google-vertexai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "58778aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports \n",
    "# # load dotenv + imports for retriever tool\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_chroma import Chroma \n",
    "from langchain.tools import tool\n",
    "import langchainhub as hub\n",
    "import chromadb\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings #type:ignore\n",
    "import vertexai\n",
    "from vertexai.evaluation import EvalTask\n",
    "from vertexai.language_models import TextEmbeddingModel\n",
    "from langchain_core.runnables import Runnable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8f1c5582",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "# Load environment variables from .env file first\n",
    "load_dotenv()\n",
    "\n",
    "# load and set environments---safe because no actual var are exposed, just using a flag method\n",
    "OPENAI_API_KEY = os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\", \"\")\n",
    "GOOGLE_GENAI_USE_VERTEXAI = os.environ[\"GOOGLE_GENAI_USE_VERTEXAI\"] = \"true\"\n",
    "GOOGLE_CLOUD_PROJECT = os.environ[\"GOOGLE_CLOUD_PROJECT\"] = \"gen-lang-client-0343643614\"\n",
    "GCLOUD_PROJECT = os.environ[\"GCLOUD_PROJECT\"] = \"gen-lang-client-0343643614\"\n",
    "GOOGLE_CLOUD_LOCATION = os.environ[\"GOOGLE_CLOUD_LOCATION\"] = \"us-east4\"\n",
    "GOOGLE_APPLICATION_CREDENTIALS = os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"/Users/briannamitchell/.config/gcp/vertex-sa.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e123bb78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GOOGLE_CLOUD_PROJECT: gen-lang-client-0343643614\n",
      "GCLOUD_PROJECT: gen-lang-client-0343643614\n",
      "GOOGLE_APPLICATION_CREDENTIALS: /Users/briannamitchell/.config/gcp/vertex-sa.json\n",
      "GOOGLE_CLOUD_LOCATION: us-east4\n",
      "GOOGLE_GENAI_USE_VERTEXAI: true\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(\"GOOGLE_CLOUD_PROJECT:\", os.getenv(\"GOOGLE_CLOUD_PROJECT\"))\n",
    "print(\"GCLOUD_PROJECT:\", os.getenv(\"GCLOUD_PROJECT\"))\n",
    "print(\"GOOGLE_APPLICATION_CREDENTIALS:\", os.getenv(\"GOOGLE_APPLICATION_CREDENTIALS\"))\n",
    "print(\"GOOGLE_CLOUD_LOCATION:\", os.getenv(\"GOOGLE_CLOUD_LOCATION\"))\n",
    "print(\"GOOGLE_GENAI_USE_VERTEXAI:\", os.getenv(\"GOOGLE_GENAI_USE_VERTEXAI\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b43dd47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables from chromaDB with persistence locally, and prevent unecessary calls to API--more cost-effective approach\n",
    "persistent_client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "\n",
    "# Importing Gemini for embedding\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"gemini-embedding-001\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53e2be3",
   "metadata": {},
   "source": [
    "# **Challenges + Mitigation strategies for Performing Vector Search:**\n",
    "---\n",
    "#### **Tradeoffs:**\n",
    "- Originally, my plan was to use the same framework for both retrievers, then fetch the data from my vector store in chroma. However, I found that my BM25 retriever simply would not return any data for either framework--langchain or llamaindex. Later on, I found out that due to how the BM25-Retriever architecture is built, the keyword search library needs access to pull data from **raw text nodes**. This does not align with my current Chroma vector store DB architecture, which is built **with stored embeddings--not original text**. Hence, why orginally no data was being returned in my prior iteration. \n",
    "\n",
    "#### **How I solved for this/Why I chose this method?:**\n",
    "To mitigate for this, I decided to change my framework and use llamaindex for BM25/lexical retriever; it provides a much more seamless integration with the Chroma vector store architecture. Then, I used llamaindex to pass my list of nodes directly to the BM25 retriever, which solved the short-term problem.\n",
    "\n",
    "#### **Improvement with Llama_Index:**\n",
    "- Use raw text nodes for BM25 retrieval to ensure compatibility with the keyword search library\n",
    "- Implement response enhancement by providing additional context or *page_content* for my document queries.\n",
    "\n",
    "---\n",
    "\n",
    "## **Pitfall/Tradeoffs with my original approach:**\n",
    "While attempting to test the combination of langchain and llamaIndex for different retrievers, I came across multiple problems. The main issue, my database and embeddings are in ChromaDB. The issue arises when performing a hybrid retrieval--where you **combine both a semantic, dense retriever and a lexical, keyword precise retriever**. The **root problem stemmed from BM25's architecture**. Semantic retriever seamlessy integrates with my dB because it uses the **vector_store from Chroma**. My BM25 does not integrate well because it **must pass the queried Document objects as nodes**--**something that can only be performed with VectorStoreIndex in LlamaIndex**. Being that Chroma is very langchain dependent and VectorStoreIndex is LlamaIndex dependant, combining these two very different methods would result in uneccessary overhead. I also had to factor in the limited constraint of having to use Gemini 1.5 with VertexAI embeddings, which is a closed-solution.\n",
    "\n",
    "- If I had to start over, to successfully integrate a hybrid retrieval RAG, I would have used either **OpenAI embeddings, ChatGPT4o, and ChromaDB** to still keep the closed-source solution in production. \n",
    "\n",
    "- If building proof of concept locally, I would use **FAISS for my vector store** + replace my **model with Gemma from HuggingFace.** for a more open source solution.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45313c8",
   "metadata": {},
   "source": [
    "# **Final RAG Approach:**\n",
    "---\n",
    "\n",
    "## My main consideration...**How can I pivot without losing retrieval quality?****\n",
    "My plan is to move to a more powerful approach by performing accurate similarity search with my **vector_store in ChromaDb** as my retriever in langchain. This is done to obtain exact matches with the potential of ingesting large documents via **Approximate Nearest Neighbors (ANN) algorithm built-in to ChromaDB**. Then, I will apply **Maximum Marginal Relevance(MMR)**. It's a great pivot from my prior solution. I'm able to optimize for balance between **relevance and diversity in a faster time complexity**. In a live healthcare environment, my agent would come in contact with multiple provider oppinions. In order to accurately generate responses that mimic live production, I would need to compensate for scaling this factor later on. Hence, whyn I chose this method.\n",
    "**Benefits:**\n",
    "- Prioritizes relevance and diversity during retrieval\n",
    "- Compensates for random spikes during data ingestion\n",
    "- Balances exploration with exploitation\n",
    "- Ensures my retrieved documents are distinct from each other"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89fb84c8",
   "metadata": {},
   "source": [
    "# **Creating VectorStore for LangChain + Enhancing Responses by Verifying Context Retrieved: How I Improved Generator Output:** \n",
    "---\n",
    "### Used response enhancement as a guideline for the model to follow when verifying retrieved content, building a better **knowledge base engine** for my agent to fetch. This system design technique results in a more enhanced **RAG performance + quality**. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "29ba2418",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding response enhancement for generated outputs\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "#get collection name from chromaDB\n",
    "#COLLECTION_NAME = \"ng12\"\n",
    "#chroma_collection = client.get_collection(name=COLLECTION_NAME)\n",
    "#embed_model = embeddings\n",
    "\n",
    "#vectore store\n",
    "vectorstore = Chroma(\n",
    "    collection_name=\"ng12\",\n",
    "    embedding_function=embeddings,\n",
    "    client=persistent_client\n",
    ")\n",
    "\n",
    "#storage context\n",
    "#storage_context = StorageContext.from_defaults(vectorstore=vectorstore)\n",
    "#storage_context.persist()\n",
    "\n",
    "documents = [\n",
    "    Document(page_content=\"Shortness of breath with cough or fatigue or chest pain or weight loss or appetite loss (unexplained), 40 and over: possible cancer Lung or mesothelioma\", metadata={\"referral\": \"Urgent\", \"source\": \"Suspected cancer: recognition and referral (NG12) 2026\", \"page\": \"52\"}),\n",
    "    Document(page_content=\"Bleeding, bruising or petechiae, unexplained: possible cancer Leukaemia\", metadata={\"referral\": \"Very urgent\", \"source\": \"Suspected cancer: recognition and referral (NG12) 2026\", \"page\": \"43\"}),\n",
    "    Document(page_content=\"Fracture unexplained, 60 and over: possible cancer Myeloma\", metadata={\"referral\": \"Unexplained\", \"source\": \"Suspected cancer: recognition and referral (NG12) 2026\", \"page\": \"55\"}),\n",
    "    Document(page_content=\"Refer people using a suspected cancer pathway referral for oesophageal cancer if they: have dysphagia or, are aged 55 and over, with weight loss, and they have any of the following: upper abdominal pain, reflux, dyspepsia. [2015, amended 2025]\", metadata={\"referral\": \"Suspected cancer pathway referral\", \"source\": \"Suspected cancer: recognition and referral (NG12) 2026\", \"page\": \"11\"}),\n",
    "    Document(page_content=\"Skin lesion that raises the suspicion of a basal cell carcinoma: possible cancer Basal cell carcinoma  \", metadata={\"referral\": \"Raises the suspicion of\", \"source\": \"Suspected cancer: recognition and referral (NG12) 2026\", \"page\": \"58\"}),\n",
    "    Document(page_content=\"Urinary urgency or frequency, increased and persistent or frequent, particularly more than 12 times per month in women, especially if 50 and over: possible cancer Ovarian\", metadata={\"referral\": \"Persistent\", \"source\": \"Suspected cancer: recognition and referral (NG12) 2026\", \"page\": \"60\"}),\n",
    "    Document(page_content=\"Upper abdominal pain with low haemoglobin levels or raised platelet count or nausea or vomiting, 55 and over: possible cancer Oesophageal or stomach \", metadata={\"referral\": \"Non-urgent\", \"source\": \"Suspected cancer: recognition and referral (NG12) 2026\", \"page\": \"40\"}),\n",
    "    Document(page_content=\"Petechiae unexplained in children and young people: possible cancer Leukaemia\", metadata={\"referral\": \"Immediate\", \"source\": \"Suspected cancer: recognition and referral (NG12) 2026\", \"page\": \"74\"})\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63265b9f",
   "metadata": {},
   "source": [
    "##### Maximum Marginal Relevance:\n",
    "---\n",
    "I use my lambda (ƛ) value as a threshold:\n",
    "- 0 = Max diversity ƛ=0\n",
    "- 1 = max relevance ƛ=1\n",
    "\n",
    "**Potential Tuning for MMR:**\n",
    "- Navigational/Exact--> ƛ= 0.7 - 0.9: \"Is this patient at 403 an urgent referral?\"\n",
    "- Balanced/Research--> ƛ= 0.5 - 0.7: \"What is the best referral recommendation based on symptoms of patient403?\"\n",
    "- Exploratory/Diverse--> ƛ= 0.3 - 0.5: \"Give me a summary of findings and determine referral status..\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e8c5813a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create retriever from vector store and then apply MMR \n",
    "\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"mmr\",\n",
    "    search_kwargs={\n",
    "        \"k\": 8,\n",
    "        \"fetch_k\": 50,\n",
    "        \"lambda_mult\": 0.6\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2e741d",
   "metadata": {},
   "source": [
    "# Techniques used(for my readme):\n",
    "- vector store as retrieval\n",
    "- response enhancement\n",
    "- MMR to diversify the output response \n",
    "- Source attribution--> helps prevent hallucinations in the model to output where information comes from\n",
    "- add Maximal marginal relevance to ensure a balance of diversity and relevance retrieved"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc721263",
   "metadata": {},
   "source": [
    "# **Perform Context Engineering**\n",
    "---\n",
    "### Tradeoffs:\n",
    "\n",
    "**Why did I choose this approach for context engineering?**\n",
    "\n",
    "**Reasoning:** \n",
    "At first, my goal was to return a single string of relevant documents. However, I realized that my agent would have trouble parsing my actual data in production. Based on the way the data is formatted in retrieval, I had to make some adjustments, and go a bit deeper into context engineering. Also, one of the key constraints in building this agent included a task to cite the specific sources within the relevant data retrieved from the NG12 documents. I could have kept my single string, however, there would have been significant fine-tuning and overhead later. Performing context engineering was the only plausible to way achieve this goal. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc77bd73",
   "metadata": {},
   "source": [
    "#### Adding Clinical Context Tool\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b41a4d5",
   "metadata": {},
   "source": [
    "# Source attribution Prompt\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ae7e25d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# attrbution\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "attribution_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\" You are a precise and accurate clinical AI assistant that provides expert knowledge in\n",
    "    clinical decision-making support for those who encounter direct patient care.\n",
    "    You have inherited a persona profile module as your agentic architecture.\n",
    "    Main Task: Your main task is to determine whether the presented patient requires an urgent referral or not.\n",
    "    Use the information provided from the text corpus below:\n",
    "    The National Institute for Health and Care Excellence (NICE) Guideline for Suspected cancer: recognition and referral NICE guideline.\n",
    "    Once, main task is complete and accurate, you must provide a recommendation that decides post-referral instructions corresponding to the most relevant medical imaging practices.\n",
    "    If patient does not meet urgent referral criteria, do not recommend any medical imaging practices.\n",
    "\n",
    "    Answer the following question based ONLY on the provided sources. \n",
    "    For each fact or claim in your answer include a citation that refers to the source.\n",
    "\n",
    "    Do not make up information or provide personal opinions in your responses without verifying answers with evidence.\n",
    "    You must cite the specific sources you found from the NICE guidelines in this specific format below:\n",
    "    This is how you are expected to format: [referral type: insert referral, source: name of text corpus, (year published), page: insert page number]\n",
    "    \n",
    "    Your source attributes at the end of your responses will look like this template below in practice:\n",
    "    [referral: Persistent, source: Suspected cancer: recognition and referral (NG12), 2026, page: 60]\n",
    "\n",
    "    How your input and output will be formatted with citation sources used:\n",
    "    Question: {question}\n",
    "\n",
    "    Sources: {sources}\n",
    "\n",
    "    Your answer:\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e90c03",
   "metadata": {},
   "source": [
    "### Helper functions to further format the sources with citation numbers and generate attributed responses\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7b7169c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#source formatted strings from documents\n",
    "def format_sources_with_citations(documents):\n",
    "    formatted_sources = []\n",
    "    for i, doc in enumerate(documents, 1):\n",
    "        source_info = f\"[{i} {doc.metadata.get('source', 'Unknown source')}]\"\n",
    "        if doc.metadata.get('page'):\n",
    "            source_info += f\", page {doc.metadata['page']}\"\n",
    "        formatted_sources.append(f\"{source_info}\\n{doc.page_content}\")\n",
    "    return \"\\n\\n\".join(formatted_sources)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858fc502",
   "metadata": {},
   "source": [
    "### Building RAG chain with my source attribution \n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f8acd626",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro\", temperature=0.0)\n",
    "\n",
    "def generated_attributed_response(question: str):\n",
    "    retrieved_docs = retriever.invoke(question)\n",
    "    sources_formatted  = format_sources_with_citations(retrieved_docs)\n",
    "    attribution_chain = attribution_prompt | llm | StrOutputParser()\n",
    "\n",
    "    response = attribution_chain.invoke({\n",
    "        \"question\": question,\n",
    "        \"sources\": sources_formatted\n",
    "    })\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeca99a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What are some symptoms that require an urgent referral?\"\n",
    "response = generated_attributed_response(question)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763b2efb",
   "metadata": {},
   "source": [
    "### Consistency Checking for Accuracy\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_stable312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
